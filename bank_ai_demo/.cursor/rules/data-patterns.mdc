---
description: Data modeling and generation Patterns and Best Practices
alwaysApply: false
---
# Data Patterns - Glacier First Bank Demo

## Overview
This rule provides patterns for data modeling, generation, and quality assurance using **PURE SQL** approach.

**Related Rules**:
- @unstructure-data-generation.mdc - For unstructured document generation patterns
- @external-data-patterns.mdc - For simulating external marketplace data
- @configuration-patterns.mdc - For schema usage standards

## CRITICAL RULES ⚠️

### 1. ALWAYS Use UNION ALL BY NAME
**Never use plain `UNION ALL` - always use `UNION ALL BY NAME`**

```sql
-- ❌ WRONG - Plain UNION ALL can cause column misalignment
SELECT col1, col2, col3 FROM table1
UNION ALL
SELECT col1, col2, col3 FROM table2

-- ✅ CORRECT - UNION ALL BY NAME matches columns by name
SELECT col1, col2, col3 FROM table1
UNION ALL BY NAME
SELECT col1, col2, col3 FROM table2
```

**Why UNION ALL BY NAME?**
- Matches columns by name, not position
- Prevents silent column misalignment errors
- More maintainable when adding/reordering columns
- Explicitly shows intent in code
- Required for reliability in Snowflake

### 2. Use Pure SQL for Structured Data Generation
**ALWAYS use pure SQL with CTEs, never Snowpark DataFrames for data generation**

Benefits:
- ✅ 6-10x faster performance
- ✅ No DataFrame API version issues
- ✅ No type mismatch errors
- ✅ No quoted identifier problems
- ✅ Easy to debug (copy SQL to worksheet)
- ✅ Standard SQL everyone can read

## Data Generation Patterns

### Structured Data Generation - PURE SQL PATTERN ⭐
```python
# PREFERRED PATTERN for all structured data generation
def generate_structured_data_pure_sql(entity_type: str, scale: str) -> None:
    """
    Generate structured data using PURE SQL with CTEs.
    This is the ONLY approved pattern for structured data generation.
    """
    import config
    
    # 1. Get scale configuration
    scale_config = config.get_scale_config(scale)
    target_count = scale_config.get(entity_type, 100)
    
    # Build key entities SQL (if applicable)
    key_entity_selects = []
    for entity_key, entity_spec in config.KEY_ENTITIES.items():
        key_entity_selects.append(f"""
            SELECT 
                '{entity_spec['id']}' AS ENTITY_ID,
                '{entity_spec['name']}' AS ENTITY_NAME,
                '{entity_spec['type']}' AS ENTITY_TYPE,
                CURRENT_TIMESTAMP() AS CREATED_DATE
        """)
    
    key_entities_union = " UNION ALL BY NAME ".join(key_entity_selects)
    additional_count = target_count - len(config.KEY_ENTITIES)
    
    # 2. Build complete SQL using CTEs
    create_table_sql = f"""
    CREATE OR REPLACE TABLE {config.SNOWFLAKE['database']}.RAW_DATA.{entity_type.upper()} AS
    WITH
    -- Key entities for demo scenarios (always include first)
    key_entities AS (
        {key_entities_union}
    ),
    -- Additional entities generated on Snowflake
    additional_entities AS (
        SELECT 
            'ENT_' || SUBSTR(UUID_STRING(), 1, 8) AS ENTITY_ID,
            CASE UNIFORM(0, 2, ROW_SEQ)
                WHEN 0 THEN 'Type A'
                WHEN 1 THEN 'Type B'
                ELSE 'Type C'
            END AS ENTITY_NAME,
            'STANDARD' AS ENTITY_TYPE,
            CURRENT_TIMESTAMP() AS CREATED_DATE
        FROM (
            SELECT ROW_NUMBER() OVER (ORDER BY SEQ4()) AS ROW_SEQ
            FROM TABLE(GENERATOR(ROWCOUNT => {additional_count}))
        )
    )
    -- Combine (ALWAYS use UNION ALL BY NAME)
    SELECT * FROM key_entities
    UNION ALL BY NAME
    SELECT * FROM additional_entities
    """
    
    # 3. Execute single SQL statement
    session.sql(create_table_sql).collect()
    
    # 4. Validate
    actual_count = session.sql(
        f"SELECT COUNT(*) FROM {config.SNOWFLAKE['database']}.RAW_DATA.{entity_type.upper()}"
    ).collect()[0][0]
    logger.info(f"Generated {actual_count} {entity_type} records using pure SQL")
```

### SQL Patterns for Common Scenarios

#### Pattern 1: GENERATOR for Bulk Data
```sql
-- Generate N rows on Snowflake
SELECT 
    ROW_NUMBER() OVER (ORDER BY SEQ4()) AS ROW_SEQ,
    UUID_STRING() AS RECORD_ID,
    UNIFORM(0, 100, SEQ4()) AS RANDOM_VALUE
FROM TABLE(GENERATOR(ROWCOUNT => 1000))
```

#### Pattern 2: CROSS JOIN for Cartesian Products
```sql
-- Generate transactions for each customer
SELECT 
    'TXN_' || c.CUSTOMER_ID || '_' || LPAD(gen.TXN_SEQ::STRING, 4, '0') AS TRANSACTION_ID,
    c.CUSTOMER_ID,
    UNIFORM(100, 10000, gen.TXN_SEQ)::DECIMAL(18,2) AS AMOUNT
FROM CUSTOMERS c
CROSS JOIN (
    SELECT ROW_NUMBER() OVER (ORDER BY SEQ4()) AS TXN_SEQ
    FROM TABLE(GENERATOR(ROWCOUNT => 100))
) gen
```

#### Pattern 3: CASE UNIFORM for Random Selection
```sql
-- Randomly select from options
SELECT 
    CASE UNIFORM(0, 4, ROW_SEQ)
        WHEN 0 THEN 'OPTION_A'
        WHEN 1 THEN 'OPTION_B'
        WHEN 2 THEN 'OPTION_C'
        WHEN 3 THEN 'OPTION_D'
        ELSE 'OPTION_E'
    END AS RANDOM_CHOICE
FROM ...
```

#### Pattern 4: Window Functions for Aggregation
```sql
-- Calculate allocation percentages
SELECT 
    CUSTOMER_ID,
    ASSET_VALUE,
    SUM(ASSET_VALUE) OVER (PARTITION BY CUSTOMER_ID) AS TOTAL_VALUE,
    (ASSET_VALUE / SUM(ASSET_VALUE) OVER (PARTITION BY CUSTOMER_ID) * 100.0)::DECIMAL(5,2) AS ALLOCATION_PCT
FROM HOLDINGS
```

#### Pattern 5: CTEs for Organization
```sql
-- Use CTEs to break down complex logic
CREATE OR REPLACE TABLE MY_TABLE AS
WITH
step1 AS (
    SELECT ...
    FROM source1
),
step2 AS (
    SELECT ...
    FROM step1
    JOIN source2 ON ...
),
step3 AS (
    SELECT ...
    FROM step2
)
SELECT * FROM step3
```

## Data Modeling Patterns

### Entity Design Pattern
```python
# Pattern for designing entities
def design_entity_table(entity_type: str) -> Dict[str, Any]:
    """Standard pattern for entity tables."""
    return {
        "primary_key": f"{entity_type}_ID",
        "required_fields": [
            f"{entity_type}_ID VARCHAR(50) PRIMARY KEY",
            f"{entity_type}_NAME VARCHAR(200) NOT NULL",
            f"{entity_type}_TYPE VARCHAR(50)",
            "CREATED_DATE TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()",
            "LAST_UPDATED TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()"
        ],
        "naming_convention": f"{ACRONYM}_{COUNTRY}_{SEQUENCE}",
        "example": "GTV_SA_001"  # Global Trade Ventures, South Africa, #001
    }
```

### Relationship Modeling Pattern
```sql
-- Pattern for many-to-many relationships
CREATE TABLE ENTITY_RELATIONSHIPS (
    RELATIONSHIP_ID VARCHAR(50) PRIMARY KEY,
    PRIMARY_ENTITY_ID VARCHAR(50) NOT NULL,
    RELATED_ENTITY_ID VARCHAR(50) NOT NULL,
    RELATIONSHIP_TYPE VARCHAR(50) NOT NULL,
    RELATIONSHIP_STRENGTH VARCHAR(20),  -- PRIMARY, SECONDARY, INDIRECT
    RISK_IMPACT_SCORE DECIMAL(3,2),     -- 0.00 to 1.00
    EFFECTIVE_DATE DATE,
    END_DATE DATE,
    CREATED_DATE TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
    FOREIGN KEY (PRIMARY_ENTITY_ID) REFERENCES ENTITIES(ENTITY_ID),
    FOREIGN KEY (RELATED_ENTITY_ID) REFERENCES ENTITIES(ENTITY_ID)
)
```

## Data Quality Patterns

### Validation Pattern
```python
def validate_data_quality(table_name: str) -> Dict[str, Any]:
    """Comprehensive data quality validation."""
    
    validations = {
        "record_count": validate_record_count(table_name),
        "null_checks": validate_required_fields(table_name),
        "foreign_keys": validate_relationships(table_name),
        "business_rules": validate_business_logic(table_name),
        "duplicates": check_duplicates(table_name)
    }
    
    # Report results
    for check, result in validations.items():
        if not result['passed']:
            logger.error(f"Validation failed for {table_name}.{check}: {result['message']}")
    
    return validations
```

### Referential Integrity Pattern
```sql
-- Check for orphaned records
SELECT COUNT(*) as orphan_count
FROM TRANSACTIONS t
LEFT JOIN CUSTOMERS c ON t.CUSTOMER_ID = c.CUSTOMER_ID
WHERE c.CUSTOMER_ID IS NULL
```

## Common Anti-Patterns ❌

### ❌ Using Plain UNION ALL
```sql
-- WRONG - Column order matters, error-prone
SELECT col1, col2, col3 FROM table1
UNION ALL
SELECT col1, col2, col3 FROM table2

-- CORRECT - Column names matched explicitly
SELECT col1, col2, col3 FROM table1
UNION ALL BY NAME
SELECT col1, col2, col3 FROM table2
```

### ❌ Using Snowpark DataFrames for Data Generation
```python
# WRONG - Slow, complex, version-dependent
df = session.create_dataframe(data)
df.with_column("new_col", F.when(...))
df.write.save_as_table(...)

# CORRECT - Fast, simple, pure SQL
session.sql("""
    CREATE OR REPLACE TABLE MY_TABLE AS
    SELECT ..., CASE WHEN ... END AS new_col
    FROM ...
""").collect()
```

### ❌ Generating Without Key Entities
```python
# WRONG - Random generation without ensuring key entities
data = generate_random_entities(count=1000)

# CORRECT - Include key entities first in SQL
"""
WITH
key_entities AS (
    SELECT ... -- Key entities
),
additional_entities AS (
    SELECT ... -- Random entities
)
SELECT * FROM key_entities
UNION ALL BY NAME
SELECT * FROM additional_entities
"""
```

### ❌ Ignoring Referential Integrity
```python
# WRONG - Generate child records before parents
generate_transactions()  # Before customers exist

# CORRECT - Follow dependency order
generate_entities()      # First
generate_customers()     # Depends on entities
generate_transactions()  # Depends on customers
```

## Performance Best Practices

1. **Use Pure SQL**: 6-10x faster than DataFrame operations
2. **Single Statement per Table**: Avoid batching and loops
3. **GENERATOR for Bulk Data**: Let Snowflake generate rows
4. **CROSS JOIN for Multiplication**: Create combinations efficiently
5. **Window Functions for Aggregation**: Calculate in SQL, not Python
6. **CTEs for Readability**: Break complex logic into steps
7. **UNIFORM for Randomness**: Use SQL random functions with seeds

## Adding New Data Types

### 1. Define Generation Function with Pure SQL
```python
def generate_new_data_type(session: Session, scale: str) -> None:
    """Generate new data type using pure SQL pattern."""
    scale_config = config.get_scale_config(scale)
    target_count = scale_config.get('new_data_type', 100)
    
    create_sql = f"""
    CREATE OR REPLACE TABLE {config.SNOWFLAKE['database']}.RAW_DATA.NEW_DATA_TYPE AS
    WITH
    key_records AS (
        -- Define key records
        SELECT ...
    ),
    additional_records AS (
        -- Generate additional records
        SELECT ...
        FROM TABLE(GENERATOR(ROWCOUNT => {target_count - key_count}))
    )
    SELECT * FROM key_records
    UNION ALL BY NAME
    SELECT * FROM additional_records
    """
    
    session.sql(create_sql).collect()
    logger.info(f"Generated {target_count} records")
```

### 2. Add Validation
```python
def validate_new_data_type(session: Session) -> Dict[str, bool]:
    """Validate new data type quality."""
    return validate_data_quality("NEW_DATA_TYPE")
```

### 3. Update Generation Order
```python
# Ensure generation order respects dependencies
GENERATION_ORDER = [
    "entities",
    "customers", 
    "new_data_type",  # Add in correct position
    "transactions"
]
```

## Best Practices Summary

1. ✅ **ALWAYS Use UNION ALL BY NAME** - Never plain UNION ALL
2. ✅ **Use Pure SQL** - No Snowpark DataFrames for data generation
3. ✅ **Key Entities First** - Include demo focal points in CTEs
4. ✅ **Respect Dependencies** - Generate parent data before children
5. ✅ **Single SQL Statement** - One CREATE OR REPLACE TABLE per table
6. ✅ **Use GENERATOR** - Let Snowflake create bulk rows
7. ✅ **Validate Everything** - Check counts, integrity, business rules
8. ✅ **Log Progress** - Report what was generated
9. ✅ **Test at Mini Scale** - Validate patterns before full generation
10. ✅ **Document with CTEs** - Make SQL self-documenting
