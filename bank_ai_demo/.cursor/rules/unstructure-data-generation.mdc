---
description: Generic unstructured data generation rules with extensible document type contracts
alwaysApply: false
---
# Generic Unstructured Data Generation Rules (Extensible)

## Purpose
This rule provides patterns for generating unstructured documents (broker research, earnings transcripts, policies, etc.) for financial services demos. It defines document type contracts, generation patterns, and quality standards that ensure realistic content aligned with demo scenarios.

## Prerequisites
- Structured data tables exist (DIM_SECURITY, DIM_ISSUER, etc.)
- Cortex Complete (LLM) access enabled
- Understanding of document types needed for demo scenarios

## CRITICAL DEPLOYMENT ORDER (Snowflake-Specific)
1. **Database/Schema/Warehouse Creation**: Must be first
2. **Structured Data Generation**: All tables with save_as_table() mode="overwrite" 
3. **Unstructured Data Generation**: Generate documents via Cortex Complete pipeline
4. **Semantic Views Creation**: ONLY after all referenced tables exist
5. **Cortex Search Services**: ONLY after document tables exist and are populated
6. **Validation**: Test search services and semantic views

## Snowflake-Specific Rules
- **CREATE OR REPLACE**: Always use for all objects (database, schema, warehouse, views, search services)
- **Reserved Keywords**: Never use STATUS, ORDER, etc. as column names (use APPLICATION_STATUS, SORT_ORDER)
- **No CREATE INDEX**: Snowflake doesn't support - use CLUSTER BY instead
- **Table Creation**: Tables created by save_as_table() don't need CREATE TABLE statements
- **Warehouse Context**: Create warehouses immediately after session creation
- **Search Service Testing**: Use simple queries without filters initially
- **Semantic View Testing**: Avoid CTEs, use simple DIMENSIONS/FACTS queries

## Token Catalogue
### Database/Schema Tokens
- {DB}: target database name
- {RAW_SCHEMA}: schema for raw generated documents
- {CURATED_SCHEMA}: schema for processed document corpora

### Generation Tokens
- {MODEL_NAME}: LLM model for content generation (e.g., llama3.1-70b)
- {LANGUAGE}: content language (e.g., en-GB, en-US)
- {GENERATION_TEMPERATURE}: LLM temperature for variability

### Business Tokens
- {COMPANY_NAME}: financial institution name for context
- {REGULATORY_REGION}: regulatory context (e.g., UK, EU, US)
- {INVESTMENT_PHILOSOPHY}: firm's investment approach

## Document Type Contracts
- SECURITY-LEVEL: requires DOCUMENT_ID, DOCUMENT_TITLE, DOCUMENT_TEXT, SecurityID, IssuerID, DOCUMENT_TYPE, PUBLISH_DATE, LANGUAGE
- ISSUER-LEVEL: requires DOCUMENT_ID, DOCUMENT_TITLE, DOCUMENT_TEXT, IssuerID, DOCUMENT_TYPE, PUBLISH_DATE, LANGUAGE
- GLOBAL: requires DOCUMENT_ID, DOCUMENT_TITLE, DOCUMENT_TEXT, DOCUMENT_TYPE, PUBLISH_DATE, LANGUAGE

## Registry (Extensible)
Define document types in config as a registry:
```yaml
DOCUMENT_TYPES:
  broker_research: { table_name: BROKER_RESEARCH_RAW, corpus_name: BROKER_RESEARCH_CORPUS, linkage_level: security,
                     word_count_range: [700, 1200] }
  earnings_transcripts: { table_name: EARNINGS_TRANSCRIPTS_RAW, corpus_name: EARNINGS_TRANSCRIPTS_CORPUS, linkage_level: security,
                          word_count_range: [6000, 10000] }
  press_releases: { table_name: PRESS_RELEASES_RAW, corpus_name: PRESS_RELEASES_CORPUS, linkage_level: security,
                    word_count_range: [250, 400] }
  # New types can be added here without code changes if they honour a contract
```

## Prompt Generation (Tokenised)
```python
def generate_prompts(session: Session, doc_type: str, entities: List[dict], test_mode: bool = False):
    cfg = config.DOCUMENT_TYPES[doc_type]
    count = (config.TEST_UNSTRUCTURED_COUNTS.get(doc_type)
             if test_mode else config.UNSTRUCTURED_COUNTS.get(doc_type))
    prompts = []
    for entity in entities[:count]:
        prompt = create_document_prompt(entity, doc_type, language=config.LANGUAGE, themes=config.THEMES)
        prompts.append({ 'prompt_id': str(uuid.uuid4()), 'document_type': doc_type,
                         'entity_id': entity.get('SecurityID') or entity.get('IssuerID'),
                         'prompt_text': prompt, 'created_date': datetime.now() })
    return prompts
```

## Content Generation (SQL pattern)
```sql
CREATE OR REPLACE TABLE {DB}.{RAW_SCHEMA}.{RAW_TABLE} AS
SELECT 
  prompt_id AS DOCUMENT_ID,
  SNOWFLAKE.CORTEX.COMPLETE('{MODEL_NAME}', prompt_text) AS DOCUMENT_TEXT,
  entity_id,
  CURRENT_DATE() AS GENERATED_DATE
FROM {DB}.{RAW_SCHEMA}.GENERATION_PROMPTS
WHERE document_type = '{DOC_TYPE}';
```

## Corpus Normalisation (Handles New Types)
```sql
-- Security-level
CREATE OR REPLACE TABLE {DB}.{CURATED_SCHEMA}.{CORPUS_TABLE} AS
SELECT 
  DOCUMENT_ID,
  REGEXP_REPLACE(SPLIT_PART(DOCUMENT_TEXT, '\n', 1), '[^a-zA-Z0-9\s\-\(\)\:]', '') AS DOCUMENT_TITLE,
  UPPER('{DOC_TYPE}') AS DOCUMENT_TYPE,
  p.entity_id AS SecurityID,
  s.IssuerID,
  GENERATED_DATE AS PUBLISH_DATE,
  '{LANGUAGE}' AS LANGUAGE,
  DOCUMENT_TEXT
FROM {DB}.{RAW_SCHEMA}.{RAW_TABLE} r
JOIN {DB}.{RAW_SCHEMA}.GENERATION_PROMPTS p USING (DOCUMENT_ID)
LEFT JOIN {DB}.{CURATED_SCHEMA}.DIM_SECURITY s ON p.entity_id = s.SecurityID
WHERE DOCUMENT_TEXT IS NOT NULL AND LENGTH(DOCUMENT_TEXT) > 100;
```

## Document Type Examples

### Security-Level Documents
**Examples**: broker_research, earnings_transcripts, press_releases
```yaml
broker_research:
  table_name: BROKER_RESEARCH_RAW
  corpus_name: BROKER_RESEARCH_CORPUS  
  linkage_level: security
  word_count_range: [700, 1200]
  sections: [executive_summary, investment_thesis, financial_analysis, risks, valuation]
```

### Issuer-Level Documents  
**Examples**: ngo_reports, engagement_notes, esg_assessments
```yaml
ngo_reports:
  table_name: NGO_REPORTS_RAW
  corpus_name: NGO_REPORTS_CORPUS
  linkage_level: issuer
  word_count_range: [400, 800]
  sections: [executive_summary, issue_background, assessment, recommendations]
```

### Global Documents
**Examples**: policy_docs, sales_templates, philosophy_docs
```yaml
policy_docs:
  table_name: POLICY_DOCS_RAW
  corpus_name: POLICY_DOCS_CORPUS
  linkage_level: global
  word_count_range: [800, 1500]
  sections: [policy_statement, scope, guidelines, monitoring]
```

## Implementation Patterns

### Document Registry Pattern
```python
# config.py
DOCUMENT_TYPES = {
    'broker_research': {
        'table_name': 'BROKER_RESEARCH_RAW',
        'corpus_name': 'BROKER_RESEARCH_CORPUS',
        'linkage_level': 'security',
        'word_count_range': [700, 1200],
        'sections': ['executive_summary', 'investment_thesis', 'financial_analysis', 'risks', 'valuation']
    },
    # Add new document types here without code changes
}
```

### Prompt Generation Pattern
```python
def generate_document_prompts(session: Session, doc_type: str, config: dict):
    """Generate prompts for document creation."""
    
    doc_config = config['DOCUMENT_TYPES'][doc_type]
    
    # Get entities based on linkage level
    if doc_config['linkage_level'] == 'security':
        entities = get_securities_for_generation(session, config)
    elif doc_config['linkage_level'] == 'issuer':
        entities = get_issuers_for_generation(session, config)
    else:
        entities = [{'id': 'GLOBAL', 'name': config['COMPANY_NAME']}]
    
    # Apply scenario coherence requirements
    if 'must_include' in config:
        entities = prioritize_entities(entities, config['must_include'])
    
    prompts = []
    for entity in entities[:config['DOCUMENT_COUNTS'][doc_type]]:
        prompt = build_prompt(entity, doc_type, doc_config, config)
        prompts.append({
            'prompt_id': str(uuid.uuid4()),
            'document_type': doc_type,
            'entity_id': entity.get('SecurityID') or entity.get('IssuerID'),
            'prompt_text': prompt,
            'created_date': datetime.now()
        })
    
    return prompts
```

### Prompt Building Pattern
```python
def build_prompt(entity: dict, doc_type: str) -> str:
    """Build document generation prompt using current config patterns."""
    import config
    
    base_prompt = f"""
    Create a professional {doc_type.replace('_', ' ')} document for {entity['ENTITY_NAME']}.
    
    Requirements:
    - Language: Professional {config.LANGUAGE} banking terminology
    - Institution: {config.INSTITUTION_NAME}
    - Currency: {config.CURRENCY}
    - Regulatory Framework: {config.REGULATORY_FRAMEWORK}
    - Entity: {entity['ENTITY_NAME']} ({entity['ENTITY_ID']})
    """
    
    # Add market themes from config
    if hasattr(config, 'MARKET_THEMES'):
        themes = [theme['name'] for theme in config.MARKET_THEMES]
        base_prompt += f"\n- Market themes: {', '.join(themes)}"
    
    # Add entity-specific context
    if doc_config['linkage_level'] == 'security':
        base_prompt += f"\n- Ticker: {entity['ticker']}, Sector: {entity['sector']}"
    
    return base_prompt
```

### Content Generation Pattern
```sql
-- SQL-based generation for efficiency
CREATE OR REPLACE TABLE {DB}.{RAW_SCHEMA}.{RAW_TABLE} AS
SELECT 
    p.prompt_id AS DOCUMENT_ID,
    SNOWFLAKE.CORTEX.COMPLETE(
        '{MODEL_NAME}',
        p.prompt_text,
        {
            'temperature': {GENERATION_TEMPERATURE},
            'max_tokens': 4000
        }
    ) AS DOCUMENT_TEXT,
    p.entity_id,
    p.document_type,
    CURRENT_DATE() AS GENERATED_DATE
FROM {DB}.{RAW_SCHEMA}.GENERATION_PROMPTS p
WHERE p.document_type = '{DOC_TYPE}';
```

### Corpus Normalisation Pattern
```python
def create_document_corpus(session: Session, doc_type: str, config: dict):
    """Create normalised corpus from raw documents."""
    
    doc_config = config['DOCUMENT_TYPES'][doc_type]
    
    # Build linkage columns based on contract
    if doc_config['linkage_level'] == 'security':
        linkage_select = "r.entity_id AS SecurityID, s.IssuerID"
        linkage_join = f"LEFT JOIN {config['DB']}.{config['CURATED_SCHEMA']}.DIM_SECURITY s ON r.entity_id = s.SecurityID"
    elif doc_config['linkage_level'] == 'issuer':
        linkage_select = "r.entity_id AS IssuerID"
        linkage_join = ""
    else:
        linkage_select = "NULL AS SecurityID, NULL AS IssuerID"
        linkage_join = ""
    
    corpus_sql = f"""
        CREATE OR REPLACE TABLE {config['DB']}.{config['CURATED_SCHEMA']}.{doc_config['corpus_name']} AS
        SELECT 
            r.DOCUMENT_ID,
            REGEXP_REPLACE(SPLIT_PART(r.DOCUMENT_TEXT, '\\n', 1), '[^a-zA-Z0-9\\s\\-\\(\\)\\:]', '') AS DOCUMENT_TITLE,
            UPPER('{doc_type}') AS DOCUMENT_TYPE,
            {linkage_select},
            r.GENERATED_DATE AS PUBLISH_DATE,
            '{config['LANGUAGE']}' AS LANGUAGE,
            r.DOCUMENT_TEXT
        FROM {config['DB']}.{config['RAW_SCHEMA']}.{doc_config['table_name']} r
        {linkage_join}
        WHERE r.DOCUMENT_TEXT IS NOT NULL 
        AND LENGTH(r.DOCUMENT_TEXT) > 100
    """
    
    session.sql(corpus_sql).collect()
```

## Quality Standards

### Content Quality Checks
```python
def validate_document_quality(session: Session, doc_type: str, config: dict):
    """Validate generated document quality."""
    
    doc_config = config['DOCUMENT_TYPES'][doc_type]
    
    # Check 1: Document count
    count_check = session.sql(f"""
        SELECT COUNT(*) as doc_count
        FROM {config['DB']}.{config['CURATED_SCHEMA']}.{doc_config['corpus_name']}
    """).collect()[0]['DOC_COUNT']
    
    # Check 2: Word count distribution
    word_stats = session.sql(f"""
        SELECT 
            AVG(ARRAY_SIZE(SPLIT(DOCUMENT_TEXT, ' '))) as avg_words,
            MIN(ARRAY_SIZE(SPLIT(DOCUMENT_TEXT, ' '))) as min_words,
            MAX(ARRAY_SIZE(SPLIT(DOCUMENT_TEXT, ' '))) as max_words
        FROM {config['DB']}.{config['CURATED_SCHEMA']}.{doc_config['corpus_name']}
    """).collect()[0]
    
    # Check 3: Linkage integrity
    if doc_config['linkage_level'] != 'global':
        linkage_check = session.sql(f"""
            SELECT COUNT(*) as unlinked
            FROM {config['DB']}.{config['CURATED_SCHEMA']}.{doc_config['corpus_name']}
            WHERE {get_linkage_column(doc_config['linkage_level'])} IS NULL
        """).collect()[0]['UNLINKED']
        
        assert linkage_check == 0, f"Found {linkage_check} unlinked documents"
    
    # Check 4: Content quality
    quality_issues = session.sql(f"""
        SELECT COUNT(*) as issues
        FROM {config['DB']}.{config['CURATED_SCHEMA']}.{doc_config['corpus_name']}
        WHERE DOCUMENT_TEXT LIKE '%I cannot%'
        OR DOCUMENT_TEXT LIKE '%I apologize%'
        OR LENGTH(DOCUMENT_TITLE) < 10
    """).collect()[0]['ISSUES']
    
    assert quality_issues == 0, f"Found {quality_issues} quality issues"
    
    print(f"✅ {doc_type}: {count_check} documents, avg {word_stats['AVG_WORDS']} words")
```

## Scenario Coherence Patterns

### Entity Prioritisation
```python
def prioritize_entities(entities: list, must_include: dict) -> list:
    """Ensure must_include entities appear first."""
    
    prioritized = []
    remaining = []
    
    must_include_figis = set(must_include.get('figi', []))
    must_include_tickers = set(must_include.get('tickers', []))
    
    for entity in entities:
        if (entity.get('FIGI') in must_include_figis or 
            entity.get('Ticker') in must_include_tickers):
            prioritized.append(entity)
        else:
            remaining.append(entity)
    
    return prioritized + remaining
```

### Theme Injection
```python
def inject_themes(prompt: str, themes: list) -> str:
    """Inject scenario themes into prompts."""
    
    theme_guidance = f"""
    IMPORTANT: This document should emphasize the following themes:
    {', '.join(themes)}
    
    Ensure these themes appear naturally in the content.
    """
    
    return prompt + theme_guidance
```

## Extensibility Guide

### Adding New Document Types
1. Add entry to DOCUMENT_TYPES registry in config
2. Define linkage_level (security/issuer/global)
3. Set word_count_range and sections
4. No code changes needed if following contracts

### Custom Document Structures
```yaml
custom_report:
  table_name: CUSTOM_REPORT_RAW
  corpus_name: CUSTOM_REPORT_CORPUS
  linkage_level: security
  word_count_range: [500, 1000]
  sections: [overview, analysis, recommendations]
  custom_prompt_template: "path/to/template.txt"  # Optional
```

## Definition of Done (Document Generation)
- [ ] Document type defined in registry
- [ ] Prompts generated for target entities
- [ ] Content generated via Cortex Complete
- [ ] Corpus table created with proper linkage
- [ ] Word count within acceptable range (±20%)
- [ ] No quality issues (apologetic responses, etc.)
- [ ] Required entities from scenario included
- [ ] Theme keywords appear in content
- [ ] Search service can index the corpus

## Current Implementation Patterns (2024 Update)

### ✅ Correct Configuration Usage

```python
def generate_compliance_documents(session: Session, scale: str = "demo") -> None:
    """Generate compliance documents using current config patterns."""
    import config
    
    # ✅ CORRECT: Use config for all constants
    entities_df = session.table(f"{config.SNOWFLAKE['database']}.RAW_DATA.ENTITIES")
    
    # ✅ CORRECT: Generate prompts with config values
    prompts = []
    for entity in entities:
        prompt_text = f"""Create documentation for {entity['ENTITY_NAME']}.
        
REQUIREMENTS:
- Language: Professional {config.LANGUAGE} banking terminology
- Institution: {config.INSTITUTION_NAME}
- Currency: {config.CURRENCY}
        """
        
        prompts.append({
            'ENTITY_NAME': entity['ENTITY_NAME'],
            'LANGUAGE': config.LANGUAGE,  # ✅ Use config variable
            'PROMPT_TEXT': prompt_text
        })
    
    # ✅ CORRECT: Proper f-string usage in SQL
    session.sql(f"""
        CREATE OR REPLACE TABLE {config.SNOWFLAKE['database']}.RAW_DATA.DOCUMENTS AS
        SELECT 
            SNOWFLAKE.CORTEX.COMPLETE('{config.LLM_MODEL}', PROMPT_TEXT) AS CONTENT,
            ENTITY_NAME
        FROM {config.SNOWFLAKE['database']}.RAW_DATA.PROMPTS
    """).collect()
```

### ❌ Anti-Patterns to Avoid

```python
# ❌ WRONG: Hardcoded values
session.sql("CREATE TABLE BANK_AI_DEMO.RAW_DATA.DOCUMENTS...")
prompts.append({'LANGUAGE': 'en-GB'})

# ❌ WRONG: Redundant f-string
session.sql(f"""FROM f"{config.SNOWFLAKE['database']}.RAW_DATA.PROMPTS""")

# ❌ WRONG: Missing config import
def generate_docs(session):
    # Hardcoded instead of config usage
    pass
```

