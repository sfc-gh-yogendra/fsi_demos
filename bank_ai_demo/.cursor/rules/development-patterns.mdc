---
description: Demo development and expansion Patterns and Best Practices
alwaysApply: false
---
# Development Patterns - Glacier First Bank Demo

## Overview
This rule provides patterns and best practices for developing and extending the Glacier First Bank demo, with emphasis on **PURE SQL data generation** and best practices.

## CRITICAL RULES ⚠️

### 1. ALWAYS Use Pure SQL for Data Generation
**DO NOT use Snowpark DataFrames** - Use pure SQL with CTEs

### 2. ALWAYS Use UNION ALL BY NAME
**Never use plain `UNION ALL`** - Always use `UNION ALL BY NAME`

### 3. Single SQL Statement Per Table
**One `CREATE OR REPLACE TABLE ... AS` statement** per table

## Code Organization Patterns

### Directory Structure
```
bank_ai_demo/
├── python/
│   ├── config.py           # Single source of configuration
│   ├── main.py            # CLI entry point
│   ├── generate_*.py      # Data generation modules (pure SQL)
│   └── create_*.py        # Object creation modules
├── sql/
│   └── archive/           # Historical SQL scripts
├── tests/
│   └── test_*.py          # Test files mirror python/ structure
├── docs/
│   ├── agent_setup.md     # Agent configuration docs
│   └── demo_scenarios.md  # Scenario documentation
└── .cursor/rules/         # Development patterns (this directory)
```

### Module Patterns

#### Data Generation Modules - PURE SQL PATTERN ⭐
```python
# PREFERRED PATTERN for data generation modules
def generate_{entity_type}(session: Session, scale: str = "demo", scenarios: List[str] = None) -> None:
    """
    Generate {entity_type} data using PURE SQL.
    
    This is the ONLY approved pattern for structured data generation.
    DO NOT use Snowpark DataFrames - use pure SQL with CTEs.
    """
    import config
    
    logger.info(f"Generating {entity_type} (pure SQL)...")
    
    # 1. Get configuration
    scale_config = config.get_scale_config(scale)
    target_count = scale_config.get('{entity_type}', 100)
    
    # 2. Build key entities if applicable
    key_entity_selects = []
    if config.KEY_ENTITIES:
        for entity_key, entity_spec in config.KEY_ENTITIES.items():
            key_entity_selects.append(f"""
                SELECT 
                    '{entity_spec['id']}' AS ID,
                    '{entity_spec['name']}' AS NAME,
                    CURRENT_TIMESTAMP() AS CREATED_DATE
            """)
        key_entities_union = " UNION ALL BY NAME ".join(key_entity_selects)
        additional_count = target_count - len(config.KEY_ENTITIES)
    else:
        key_entities_union = ""
        additional_count = target_count
    
    # 3. Build complete SQL using CTEs
    create_sql = f"""
    CREATE OR REPLACE TABLE {config.SNOWFLAKE['database']}.RAW_DATA.{entity_type.upper()} AS
    WITH
    {"-- Key records for demo scenarios" if key_entities_union else ""}
    {"key_records AS (" + key_entities_union + ")," if key_entities_union else ""}
    {"" if not key_entities_union else "-- "}Additional records generated on Snowflake
    additional_records AS (
        SELECT 
            'GEN_' || SUBSTR(UUID_STRING(), 1, 8) AS ID,
            CASE UNIFORM(0, 2, ROW_SEQ)
                WHEN 0 THEN 'Type A'
                WHEN 1 THEN 'Type B'
                ELSE 'Type C'
            END AS NAME,
            CURRENT_TIMESTAMP() AS CREATED_DATE
        FROM (
            SELECT ROW_NUMBER() OVER (ORDER BY SEQ4()) AS ROW_SEQ
            FROM TABLE(GENERATOR(ROWCOUNT => {additional_count}))
        )
    )
    -- Combine (ALWAYS use UNION ALL BY NAME)
    {"SELECT * FROM key_records" if key_entities_union else ""}
    {"UNION ALL BY NAME" if key_entities_union else ""}
    SELECT * FROM additional_records
    """
    
    # 4. Execute single SQL statement
    session.sql(create_sql).collect()
    
    # 5. Log results
    logger.info(f"Generated {target_count} {entity_type} records using pure SQL")


# Benefits of Pure SQL Pattern:
# ✅ 6-10x faster performance
# ✅ No DataFrame API version issues
# ✅ No type mismatch errors
# ✅ No quoted identifier problems
# ✅ Easy to debug (copy SQL to worksheet)
# ✅ Standard SQL everyone can read
```

#### Object Creation Modules
```python
# Pattern for creating Snowflake objects
def create_{object_type}(session: Session) -> None:
    """Create {object_type} in Snowflake."""
    import config
    
    logger.info(f"Creating {object_type}...")
    
    # 1. Validate prerequisites
    validate_required_objects(session, prerequisites)
    
    # 2. Use CREATE OR REPLACE
    session.sql(f"""
        CREATE OR REPLACE {object_type} {config.SNOWFLAKE['database']}.{schema}.{name}
        ...
    """).collect()
    
    # 3. Validate creation
    logger.info(f"Created {object_type} successfully")
```

## Error Handling Patterns

### Connection Errors
```python
def get_snowflake_session(connection_name: str) -> Session:
    """Create Snowflake session with proper error handling."""
    try:
        return Session.builder.config("connection_name", connection_name).create()
    except Exception as e:
        if "connection" in str(e).lower():
            raise ValueError(
                f"Failed to connect to Snowflake with connection '{connection_name}'. "
                f"Please verify your connection configuration."
            )
        raise
```

### Data Validation Errors
```python
def validate_required_tables(session: Session, tables: List[str]) -> None:
    """Validate required tables exist before proceeding."""
    import config
    
    missing_tables = []
    for table in tables:
        try:
            count = session.sql(
                f"SELECT COUNT(*) FROM {config.SNOWFLAKE['database']}.RAW_DATA.{table}"
            ).collect()[0][0]
            if count == 0:
                logger.warning(f"Table {table} exists but is empty")
        except Exception:
            missing_tables.append(table)
    
    if missing_tables:
        raise RuntimeError(
            f"Required tables missing: {', '.join(missing_tables)}. "
            f"Run data generation first: python main.py generate-structured"
        )
```

### LLM Generation Errors
```python
def generate_content_with_retry(session: Session, prompt: str, max_retries: int = 3) -> str:
    """Generate content with exponential backoff retry."""
    import config
    import time
    
    for attempt in range(max_retries):
        try:
            result = session.sql(f"""
                SELECT SNOWFLAKE.CORTEX.COMPLETE(
                    '{config.LLM_MODEL}', 
                    $${prompt}$$
                )
            """).collect()
            
            content = result[0][0]
            if content and len(content) > 100:
                return content
                
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # Exponential backoff
                logger.warning(f"Retry {attempt + 1}/{max_retries} after {wait_time}s: {str(e)}")
                time.sleep(wait_time)
            else:
                raise RuntimeError(f"Failed after {max_retries} attempts: {str(e)}")
```

## Performance Optimization Patterns

### Pure SQL Data Generation (Fastest) ⭐
```python
# Use pure SQL for maximum performance
def generate_transactions(session: Session, scale: str) -> None:
    """Generate transactions using pure SQL - 20-50x faster than DataFrame approach."""
    scale_config = config.get_scale_config(scale)
    
    # Single SQL statement generates all data on Snowflake
    create_sql = f"""
    CREATE OR REPLACE TABLE {config.SNOWFLAKE['database']}.RAW_DATA.TRANSACTIONS AS
    SELECT 
        'TXN_' || c.CUSTOMER_ID || '_' || LPAD(gen.TXN_SEQ::STRING, 4, '0') AS TRANSACTION_ID,
        c.CUSTOMER_ID,
        DATEADD(day, -UNIFORM(1, 365, gen.TXN_SEQ), CURRENT_TIMESTAMP()) AS TRANSACTION_DATE,
        UNIFORM(1000, 100000, gen.TXN_SEQ + 1)::DECIMAL(18,2) AS AMOUNT,
        'EUR' AS CURRENCY
    FROM {config.SNOWFLAKE['database']}.RAW_DATA.CUSTOMERS c
    CROSS JOIN (
        SELECT ROW_NUMBER() OVER (ORDER BY SEQ4()) AS TXN_SEQ
        FROM TABLE(GENERATOR(ROWCOUNT => 100))
    ) gen
    WHERE (c.RISK_RATING = 'HIGH' AND gen.TXN_SEQ <= 100)
       OR (c.RISK_RATING = 'MEDIUM' AND gen.TXN_SEQ <= 60)
       OR (c.RISK_RATING = 'LOW' AND gen.TXN_SEQ <= 30)
    """
    
    session.sql(create_sql).collect()
```

### Parallel Generation
```python
# Pattern for parallel data generation
def generate_documents_parallel(session: Session, doc_types: List[str]) -> None:
    """Generate multiple document types in parallel."""
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            executor.submit(generate_documents, session, doc_type): doc_type
            for doc_type in doc_types
        }
        
        for future in as_completed(futures):
            doc_type = futures[future]
            try:
                future.result()
                logger.info(f"Completed generation for {doc_type}")
            except Exception as e:
                logger.error(f"Failed to generate {doc_type}: {str(e)}")
```

## Logging and Monitoring Patterns

### Structured Logging
```python
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('glacier_demo.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Pattern for operation logging
def log_operation(operation: str, entity_type: str, count: int):
    """Log operations with consistent format."""
    logger.info(f"{operation} | {entity_type} | Count: {count} | Time: {datetime.now()}")
```

### Performance Monitoring
```python
# Pattern for timing operations
from functools import wraps
import time

def timed_operation(operation_name: str):
    """Decorator to time operations."""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                duration = time.time() - start_time
                logger.info(f"{operation_name} completed in {duration:.2f}s")
                return result
            except Exception as e:
                duration = time.time() - start_time
                logger.error(f"{operation_name} failed after {duration:.2f}s: {str(e)}")
                raise
        return wrapper
    return decorator

# Usage
@timed_operation("Data Generation")
def generate_all_data(session: Session, scale: str):
    # ... implementation ...
```

## Common Anti-Patterns to Avoid ❌

### ❌ Using Snowpark DataFrames for Data Generation
```python
# WRONG - Slow, complex, version-dependent
data = []
for i in range(1000):
    data.append({"id": i, "value": random.random()})
df = session.create_dataframe(data)
df.write.save_as_table("MY_TABLE")

# CORRECT - Fast, simple, pure SQL
session.sql("""
    CREATE OR REPLACE TABLE MY_TABLE AS
    SELECT 
        ROW_NUMBER() OVER (ORDER BY SEQ4()) AS ID,
        UNIFORM(0, 100, SEQ4()) / 100.0 AS VALUE
    FROM TABLE(GENERATOR(ROWCOUNT => 1000))
""").collect()
```

### ❌ Using Plain UNION ALL
```python
# WRONG - Column position matters, error-prone
sql = """
    SELECT col1, col2 FROM table1
    UNION ALL
    SELECT col1, col2 FROM table2
"""

# CORRECT - Column names matched explicitly
sql = """
    SELECT col1, col2 FROM table1
    UNION ALL BY NAME
    SELECT col1, col2 FROM table2
"""
```

### ❌ Hardcoding Values
```python
# WRONG
session.sql("CREATE DATABASE BANK_AI_DEMO")

# CORRECT
session.sql(f"CREATE DATABASE {config.SNOWFLAKE['database']}")
```

### ❌ Silent Failures
```python
# WRONG
try:
    risky_operation()
except:
    pass  # Never do this

# CORRECT
try:
    risky_operation()
except SpecificException as e:
    logger.error(f"Operation failed: {str(e)}")
    raise  # Or handle appropriately
```

## Development Workflow

### Adding New Features
1. Update configuration in `config.py`
2. Create generation function using pure SQL pattern
3. Update object creation scripts
4. Add tests
5. Update documentation
6. Run validation suite

### Pre-commit Checklist
- [ ] All tests pass
- [ ] No hardcoded values
- [ ] Pure SQL used for data generation
- [ ] UNION ALL BY NAME used everywhere
- [ ] Proper error handling
- [ ] Logging added for operations
- [ ] Configuration documented
- [ ] SQL uses CREATE OR REPLACE

### Debugging Checklist
- [ ] Check connection configuration
- [ ] Verify deployment order followed
- [ ] Check Snowflake query history
- [ ] Review error logs
- [ ] Validate data exists in tables
- [ ] Test with mini scale first
- [ ] Copy SQL to worksheet for debugging

## Performance Best Practices

1. ✅ **Use Pure SQL**: 6-10x faster than DataFrame operations
2. ✅ **Single Statement**: One `CREATE OR REPLACE TABLE` per table
3. ✅ **GENERATOR for Bulk**: Let Snowflake generate rows
4. ✅ **CROSS JOIN for Cartesian**: Create combinations efficiently
5. ✅ **Window Functions**: Calculate aggregations in SQL
6. ✅ **CTEs for Organization**: Break complex logic into steps
7. ✅ **UNION ALL BY NAME**: Ensure column alignment
8. ✅ **UNIFORM for Random**: Use SQL random with seeds
9. ✅ **Avoid Python Loops**: Do everything in SQL
10. ✅ **Log Performance**: Track execution times
