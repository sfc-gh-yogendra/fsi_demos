---
alwaysApply: true
---

# SAM Demo - 100% Real Asset Implementation Rules

Complete implementation using authentic financial instrument data from Snowflake Marketplace OpenFIGI dataset. **No synthetic securities generated** - all 14,000+ securities are real with authentic Bloomberg FIGI identifiers.

## Configuration

### Settings in `python/config.py`
```python
# Real asset data (required - no synthetic fallback)
USE_REAL_ASSETS_CSV = True  # REQUIRED: Must be True for real-only mode
REAL_ASSETS_CSV_PATH = './data/real_assets.csv'  # Path to OpenFIGI dataset
EXTRACT_REAL_ASSETS = False  # Set to True to extract from Marketplace

# Data volumes (real assets only) - based on available capacity
SECURITIES_COUNT = {'equities': 10000, 'bonds': 3000, 'etfs': 1000}  # All real assets

# Marketplace data source (requires subscription)
MARKETPLACE_DATABASE = 'FINANCIALS_ECONOMICS_ENTERPRISE'
OPENFIGI_SCHEMA = 'CYBERSYN'
```

## CLI Usage

### Extract Real Assets
```bash
# Extract real assets from Snowflake Marketplace (requires Marketplace access)
python main.py --extract-real-assets

# Build with 100% real assets (default mode)
python main.py --scenarios portfolio_copilot
```

## Implementation Pattern

### Error Handling Logic (Real-Only Mode)
```python
# In generate_structured.py - real assets required, no fallback
if not config.USE_REAL_ASSETS_CSV:
    raise Exception("Real assets CSV required - set USE_REAL_ASSETS_CSV = True in config.py")

if not os.path.exists(config.REAL_ASSETS_CSV_PATH):
    raise Exception(f"Real assets CSV not found at {config.REAL_ASSETS_CSV_PATH} - run 'python main.py --extract-real-assets' first")

print("‚úÖ Using real asset data for securities (100% authentic mode)")
```

### Critical Implementation Rules

#### 1. **100% Real Assets Implementation**
- **NO SYNTHETIC SECURITIES**: All securities must come from real OpenFIGI dataset
- **Scale**: 14,000+ real securities (10K equities + 3K bonds + 1K ETFs)
- **Identifiers**: TICKER + authentic Bloomberg FIGI only
- **Extraction**: Remove all LIMIT clauses from marketplace queries for complete coverage
- **Error on Missing CSV**: Hard error if real assets CSV not found - no synthetic fallback

#### 2. **Handle Pandas NaN Values in String Operations**
```python
# ‚ùå WRONG - Will fail with 'float' object is not subscriptable
country = asset.get('COUNTRY_OF_DOMICILE', 'DE')[:2]

# ‚úÖ CORRECT - Safe handling of NaN/float values  
country_of_domicile = asset.get('COUNTRY_OF_DOMICILE')
if country_of_domicile and not pd.isna(country_of_domicile) and isinstance(country_of_domicile, str):
    safe_country = country_of_domicile[:2]
else:
    safe_country = 'US'  # Default
```

#### 3. **Portfolio Holdings Alignment** 
- Ensure portfolio holdings prioritize major real stocks for demo coherence
- Use consistent prioritization in both `build_fact_transaction()` and `generate_unstructured.py`
- **Priority Order**: Major US stocks (AAPL, MSFT, NVDA, etc.) ‚Üí Other real securities
- **Coverage**: All 14,000 real securities available for portfolio allocation

#### 4. **Demo Flow Coherence**
- Portfolio holdings must align with demo scenario questions
- Update demo scenarios to use actual portfolio holdings, not aspirational stocks
- Test actual holdings before updating demo documentation

### Enhanced SQL Query (No Limits)
```sql
-- Enhanced query in extract_real_assets.py
-- Key changes: NO LIMIT clause, expanded asset categories, includes ISSUER_NAME
SELECT
    cs.MARKET_REGION,
    cs.ASSET_CATEGORY,
    cs.ISSUER_NAME,           -- New: For issuer dimension building
    cs.INDUSTRY_SECTOR,
    cs.TOP_LEVEL_OPENFIGI_ID,
    cs.SECURITY_NAME,
    cs.PRIMARY_TICKER,
    cs.PRIMARY_EXCHANGE_CODE,
    cs.PRIMARY_EXCHANGE_NAME,
    cs.COUNTRY_OF_DOMICILE,
    cs.EXCHANGE_CODES
FROM Categorized_Securities cs
WHERE cs.ASSET_CATEGORY IS NOT NULL
ORDER BY cs.MARKET_REGION, cs.ASSET_CATEGORY, cs.ISSUER_NAME, cs.SECURITY_NAME
-- NO LIMIT - capture ALL assets including USA
```

### OpenFIGI Query Pattern
```python
# In extract_real_assets.py
def extract_real_assets_to_csv(session: Session):
    """Extract real assets using OpenFIGI standard"""
    # Uses enhanced SQL with expanded coverage
    # Extracts ALL securities across USA/EU/APAC regions
    # Includes expanded asset classes: Equity/Corp Bond/Govt Bond/Muni/ETF
    # Maps company relationships for issuer dimension
    # NO LIMIT ensures complete coverage including USA securities
```

## Data Source Requirements

### Marketplace Prerequisites
- **Dataset**: "Public Data Financials & Economics: Enterprise"
- **Database**: `FINANCIALS_ECONOMICS_ENTERPRISE.CYBERSYN`
- **Tables**: 
  - `OPENFIGI_SECURITY_INDEX` (security master data)
  - `COMPANY_SECURITY_RELATIONSHIPS` (company linkages)
  - `STOCK_PRICE_TIMESERIES` (real daily OHLCV data)
- **Compute**: Sufficient warehouse for complex query execution

### Fallback Strategy (Current Working)
- Real major tickers: AAPL, MSFT, NVDA, ASML, TSM, NESTLE, etc.
- Proper geographic distribution: 55% US, 30% EU, 15% APAC/EM
- Correct asset classes: 70% Equity, 20% Bonds, 10% ETF
- Realistic characteristics: Bond ratings, ETF structures

## Tested Results

### Successful Extraction
- ‚úÖ **Total Assets**: 6,163 real securities extracted
- ‚úÖ **Distribution**: 3,229 APAC/EM Equity, 2,934 EU Equity
- ‚úÖ **File Size**: 1.7MB CSV with complete metadata
- ‚úÖ **Geographic Coverage**: Authentic global market representation

### Benefits of Real Assets
- Authentic ticker symbols for enhanced demo credibility
- Proper industry sector classifications from SIC codes
- Real geographic distribution across global markets
- Enhanced customer presentation authenticity

## Real Asset Data Processing Optimization Patterns

### Optimization Strategy Overview
**Status**: ‚úÖ **FULLY IMPLEMENTED** - All main real asset functions optimized

**Core Principles**:
1. **Snowpark First**: Use Snowpark operations when possible for best performance
2. **Hybrid Approach**: Combine optimized pandas filtering with Snowpark DataFrame creation when needed
3. **Fallback Strategy**: Always provide synthetic alternatives when real data unavailable
4. **Error Resilience**: Handle pandas NaN values and missing data gracefully

### Function Optimization Patterns

#### Pattern 1: Full Snowpark Optimization (Recommended)
**Use Case**: When working with temporary tables and simple column operations
**Example**: `build_dim_issuer_from_real_data`

```python
def build_optimized_function_snowpark(session: Session):
    """Pattern for full Snowpark optimization"""
    
    # Load real assets to temporary table
    real_assets_df = session.write_pandas(
        real_assets_df_pandas,
        table_name="TEMP_REAL_ASSETS",
        quote_identifiers=False,
        auto_create_table=True, 
        table_type="temp"
    )
    
    # Use Snowpark DataFrame operations
    from snowflake.snowpark.functions import (
        col, lit, ifnull, row_number, abs as abs_func, hash as hash_func,
        substr, trim, to_varchar, concat
    )
    from snowflake.snowpark import Window
    
    result_df = (real_assets_df.select(
        ifnull(col("ISSUER_NAME"), col("SECURITY_NAME")).alias("LegalName"),
        ifnull(col("COUNTRY_OF_DOMICILE"), lit("US")).alias("CountryOfIncorporation"),
        col("INDUSTRY_SECTOR").alias("GICS_Sector")
    )
    .filter((col("LegalName").isNotNull()) & (col("LegalName") != lit("Unknown")))
    .distinct()
    .select(
        row_number().over(Window.order_by("LegalName")).alias("ID"),
        # ... other transformations
    ))
    
    # Save to database
    result_df.write.mode("overwrite").save_as_table("TARGET_TABLE")
```

#### Pattern 2: Optimized Pandas + Snowpark Hybrid (Fallback)
**Use Case**: When Snowpark quoted identifier issues arise or complex pandas operations needed
**Example**: `build_dim_security_from_real_data`

```python
def build_optimized_function_hybrid(session: Session, targets: dict):
    """Pattern for optimized pandas + Snowpark hybrid"""
    
    # Load real assets using pandas (efficient for complex filtering)
    from extract_real_assets import load_real_assets_from_csv
    real_assets_df = load_real_assets_from_csv()
    
    # Optimized pandas operations (vectorized, no iteration)
    all_data = []
    for category, target_count in targets.items():
        # Efficient pandas filtering with vectorized operations
        category_assets = (real_assets_df[
            (real_assets_df['ASSET_CATEGORY'] == category) &
            (real_assets_df['PRIMARY_TICKER'].notna()) &
            (real_assets_df['PRIMARY_TICKER'].str.len() <= 10) &
            (~real_assets_df['PRIMARY_TICKER'].str.contains(' ', na=False))
        ].drop_duplicates(subset=['PRIMARY_TICKER'], keep='first')
         .head(target_count))
        
        # Process in batches, not row-by-row
        for _, asset in category_assets.iterrows():
            all_data.append({
                'ID': len(all_data) + 1,
                'Ticker': asset['PRIMARY_TICKER'],
                'Description': str(asset.get('SECURITY_NAME', asset['PRIMARY_TICKER']))[:255],
                # ... other fields with safe NaN handling
            })
    
    # Use Snowpark for database operations
    if all_data:
        result_df = session.create_dataframe(all_data)
        result_df.write.mode("overwrite").save_as_table("TARGET_TABLE")
```

#### Pattern 3: Safe NaN Handling (MANDATORY)
**Critical for all real asset processing**:

```python
# ‚úÖ CORRECT - Safe handling patterns
def safe_string_operations(asset_row):
    """Safe handling of pandas NaN values in string operations"""
    
    # For string slicing
    country = asset_row.get('COUNTRY_OF_DOMICILE')
    if country and not pd.isna(country) and isinstance(country, str):
        safe_country = country[:2]
    else:
        safe_country = 'US'  # Default
    
    # For string methods
    industry = asset_row.get('INDUSTRY_SECTOR')
    if industry and not pd.isna(industry) and isinstance(industry, str):
        safe_industry = industry.lower()
    else:
        safe_industry = 'diversified'
    
    # For string formatting
    description = str(asset_row.get('SECURITY_NAME', asset_row['PRIMARY_TICKER']))[:255]
    
    return safe_country, safe_industry, description
```

### Implementation Status (COMPLETED)

#### ‚úÖ Optimized Functions
1. **`build_dim_issuer_from_real_data`** - Full Snowpark optimization with temp table reuse
2. **`build_dim_security_from_real_data`** - Optimized pandas + Snowpark hybrid approach
3. **`build_marketdata_synthetic`** - Efficient SQL-based synthetic market data generation
4. **ESG/Factor/Benchmark Functions** - Implemented with efficient SQL generation

#### ‚úÖ Performance Results
- **Issuer Generation**: 79,186 real issuers from 92,573 assets using Snowpark operations
- **Security Generation**: 400 real securities (350 Equity + 50 ETF) using optimized pandas
- **Market Data**: 4M+ records with synthetic generation for all securities
- **ESG/Factor Data**: Full SQL-based generation with sector-specific characteristics

#### ‚úÖ Demo Flow Alignment (RESOLVED)
- **Issue**: Portfolio holdings didn't align with research coverage
- **Solution**: Prioritized major US stocks (AAPL, MSFT, NVDA, etc.) in both:
  - Transaction generation (`build_fact_transaction`)
  - Document generation (`generate_unstructured.py`)
- **Result**: Coherent demo flow with aligned holdings and research

### Snowpark Quoted Identifier Issue (DOCUMENTED)
**Issue**: `session.write_pandas()` creates quoted column identifiers even with `quote_identifiers=False`
**Impact**: Subsequent Snowpark operations fail with "invalid identifier" errors
**Workaround**: Use optimized pandas + Snowpark hybrid approach when this occurs
**Future**: Investigate Snowflake/Snowpark version compatibility for resolution

### Real Asset Integration Best Practices

#### 1. **Function Structure Template**
```python
def build_real_asset_function(session: Session, targets: dict):
    """Template for real asset integration functions"""
    
    # Step 1: Load real assets (use TEMP_REAL_ASSETS pattern if multiple functions need data)
    # See TEMP_REAL_ASSETS Table Reuse Pattern section above for optimization
    if check_temp_real_assets_exists(session):
        print("‚úÖ Reusing existing TEMP_REAL_ASSETS table (optimization)")
        real_assets_df = session.table("TEMP_REAL_ASSETS").to_pandas()
    else:
        print("üì• Loading real assets from CSV")
        try:
            from extract_real_assets import load_real_assets_from_csv
            real_assets_df = load_real_assets_from_csv()
            
            if real_assets_df is None:
                raise Exception("Real assets CSV not found - required for real-only mode")
                
        except Exception as e:
            print(f"‚ùå Error loading real assets: {e}")
            print("   To fix: Run 'python main.py --extract-real-assets' first")
            raise
    
    # Step 2: Process real assets (use appropriate pattern)
    # Pattern 1: Full Snowpark or Pattern 2: Hybrid approach
    
    # Step 3: Process all data using real assets only
    
    # Step 4: Validate and report results
```

#### 2. **Error Resilience Requirements** 
- Hard error when real data unavailable (no synthetic fallback)
- Handle pandas NaN values with explicit checks
- Use try/catch blocks around real asset operations
- Report actual counts and authentic data utilization for transparency

#### 3. **Performance Optimization Rules**
- **Prefer SQL generation** for large datasets when possible
- **Use vectorized pandas operations** instead of row-by-row iteration
- **Batch database operations** using `session.create_dataframe()`
- **Leverage temp tables** for intermediate processing when appropriate
- **Reuse TEMP_REAL_ASSETS table** across multiple functions to avoid duplicate CSV loading

### Future Real Asset Integration

#### TEMP_REAL_ASSETS Table Reuse Pattern (MANDATORY)
**Use Case**: When multiple functions need real asset data in sequence
**Benefit**: Eliminates duplicate CSV loading (92,573 records, ~24MB file)

```python
# Helper function (add once to module)
def check_temp_real_assets_exists(session: Session) -> bool:
    """Check if TEMP_REAL_ASSETS table exists in the session."""
    try:
        session.sql("SELECT 1 FROM TEMP_REAL_ASSETS LIMIT 1").collect()
        return True
    except Exception:
        return False

# First function: Creates temp table
def build_first_function_from_real_data(session: Session):
    """First function that loads real assets and creates temp table."""
    
    # Load real assets from CSV
    from extract_real_assets import load_real_assets_from_csv
    real_assets_df_pandas = load_real_assets_from_csv()
    
    # Create temp table for reuse by other functions
    real_assets_df = session.write_pandas(
        real_assets_df_pandas,
        table_name="TEMP_REAL_ASSETS",
        quote_identifiers=False,
        auto_create_table=True, 
        table_type="temp"
    )
    print("‚úÖ Created TEMP_REAL_ASSETS table for reuse by other functions")
    
    # Process data...

# Subsequent functions: Reuse temp table
def build_subsequent_function_from_real_data(session: Session):
    """Subsequent function that reuses existing temp table."""
    
    # Check if TEMP_REAL_ASSETS table already exists
    if check_temp_real_assets_exists(session):
        print("‚úÖ Reusing existing TEMP_REAL_ASSETS table (optimization)")
        # Load from existing temp table
        real_assets_df = session.table("TEMP_REAL_ASSETS").to_pandas()
    else:
        print("üì• Loading real assets from CSV (TEMP_REAL_ASSETS not found)")
        # Fallback: Load from CSV
        from extract_real_assets import load_real_assets_from_csv
        real_assets_df = load_real_assets_from_csv()
    
    # Process data...
```

**Implementation Rules**:
1. **First function** in execution order creates `TEMP_REAL_ASSETS` table
2. **Subsequent functions** check for existing table before loading CSV
3. **Always provide fallback** to CSV loading if temp table doesn't exist
4. **Use clear logging** to indicate optimization vs fallback paths
5. **Maintain error handling** for both temp table and CSV loading scenarios

**Execution Order Dependency**:
- Current implementation: `build_dim_issuer` ‚Üí `build_dim_security`
- First function (`build_dim_issuer`) creates temp table
- Second function (`build_dim_security`) reuses temp table
- **Important**: If execution order changes, update which function creates the temp table

#### Adding New Real Asset Functions
When adding new functions that use real asset data:

1. **Use TEMP_REAL_ASSETS Pattern**: Follow the mandatory temp table reuse pattern above
2. **Choose Optimization Pattern**: Start with Pattern 1 (Full Snowpark), fallback to Pattern 2 if needed
3. **Implement Safe NaN Handling**: Use established patterns for string operations
4. **Test End-to-End**: Verify with actual CSV data and missing CSV scenarios
5. **Document Performance**: Report data volumes and processing efficiency

#### Adding New Real Data Sources
When integrating additional real data sources:

1. **Follow CSV Pattern**: Store in `PROJECT_ROOT/data/` directory
2. **Add Configuration**: Update `config.py` with new paths and flags
3. **Implement Extraction**: Create extraction function in `extract_real_assets.py`
4. **Add CLI Support**: Update `main.py` with extraction flag
5. **Create Optimization**: Apply appropriate optimization pattern to processing functions

## Market Data Generation (SYNTHETIC ONLY)

### Current Status
**Status**: ‚úÖ Synthetic market data generation for all securities

**Configuration**:
```python
# Market data configuration (in config.py)
# All market data is generated synthetically for all securities
```

**Working Commands**:
```bash
# Build with synthetic market data for all securities (default mode)
python main.py --scenarios portfolio_copilot
```

### Implementation Results
- ‚úÖ **Generation**: Synthetic OHLCV data for all 14,000+ securities
- ‚úÖ **Consistency**: Uniform data quality across all securities  
- ‚úÖ **Performance**: Excellent performance with synthetic-only approach
- ‚úÖ **Simplicity**: No external data dependencies or hybrid logic
- ‚úÖ **Benefits**: Realistic volatility patterns and consistent market behavior

## Implementation Status (100% REAL ASSETS COMPLETE)

### Phase 1: Real Asset Extraction ‚úÖ
**File**: `python/extract_real_assets.py`
- ‚úÖ Complete OpenFIGI dataset extraction (92,573 securities)
- ‚úÖ Comprehensive asset coverage: Equity, Corporate Bond, ETF, Government Bond, Municipal Bond
- ‚úÖ Global coverage: USA, EU, APAC/EM regions
- ‚úÖ Authentic Bloomberg FIGI identifiers for all securities

### Phase 2: 100% Real Implementation ‚úÖ
**File**: `python/generate_structured.py`
- ‚úÖ **NO SYNTHETIC SECURITIES**: All 14,000+ securities from real data
- ‚úÖ `build_dim_security_from_real_data()` - real-only mode with error on missing CSV
- ‚úÖ Enhanced filtering: Asset-category-specific logic for quality
- ‚úÖ **Scale**: 10,000 equities + 3,000 bonds + 1,000 ETFs
- ‚úÖ Authentic identifiers: TICKER + real Bloomberg FIGI only

### Phase 3: Real-Only Configuration ‚úÖ
**File**: `python/config.py` 
- ‚úÖ `SECURITIES_COUNT = {'equities': 10000, 'bonds': 3000, 'etfs': 1000}`
- ‚úÖ `USE_REAL_ASSETS_CSV = True` (required - no synthetic fallback)
- ‚úÖ Realistic targets based on available real data capacity

### Phase 4: Validation & Quality ‚úÖ
**Results**: 14,000 securities with 100% authentic Bloomberg identifiers
- ‚úÖ **Authenticity**: All FIGI identifiers start with "BBG" (real Bloomberg codes)
- ‚úÖ **Scale**: 28x increase from 500 to 14,000 securities
- ‚úÖ **Coverage**: 3,303 real issuers across global markets
- ‚úÖ **Integration**: 27,000+ portfolio holdings using real securities
- ‚úÖ **AI Compatibility**: All semantic views and agents work unchanged

### Key Implementation Lessons

#### Data Source Challenges Solved
- **Issue**: Real assets CSV only contained APAC/EM and EU regions, no USA entries
- **Solution**: Modified extraction to use all equity regions, filter by ticker format
- **Learning**: Always validate data structure before implementing extraction logic

#### Query Optimization Required
- **Issue**: Initial query too restrictive (exchange filters, asset class, date range)
- **Solution**: Simplified query, reduced date range to 2 years, removed strict filters
- **Learning**: Start with broad queries and narrow down based on data availability

#### Integration Architecture
- **Pattern**: Use temporary tables for efficient real data processing
- **Approach**: `COALESCE()` function for seamless real/synthetic blending
- **Fallback**: Always provide synthetic alternative when real data unavailable
- **Statistics**: Report real vs synthetic data usage for transparency

### Working Commands (100% Real Mode)
```bash
# Extract real assets (one-time setup - requires Marketplace access)
python main.py --extract-real-assets

# Build with 100% real assets (default mode)
python main.py --scenarios portfolio_copilot

# Test mode with 100% real assets (1,400 securities)
python main.py --test-mode --scenarios portfolio_copilot

# Full scale with 100% real assets (14,000 securities)
python main.py --scenarios all

# Check real assets file
ls -la data/real_assets.csv
```

### Validated Results (100% Real Implementation)
- ‚úÖ **Scale**: 14,000 real securities vs. 500 mixed securities previously
- ‚úÖ **Authenticity**: 100% Bloomberg FIGI identifiers (all start with "BBG")  
- ‚úÖ **Coverage**: 3,303 real issuers across Equity/Bond/ETF asset classes
- ‚úÖ **Integration**: 27,000+ portfolio holdings using authentic securities
- ‚úÖ **Performance**: Excellent performance with real-only approach
- ‚úÖ **AI Compatibility**: All agents and demos work unchanged