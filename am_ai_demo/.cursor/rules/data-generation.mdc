---
alwaysApply: true
---

# SAM Demo - 100% Real Asset Data Model Guide

Complete guide for the industry-standard data model using 100% authentic securities from Snowflake Marketplace OpenFIGI dataset. **No synthetic securities generated.**

## Overview

This guide covers the enhanced data model with 14,000+ real securities, immutable SecurityID, transaction-based holdings, and corporate hierarchy support using synthetic market data for consistent performance.

**Key Principles:**
- **100% Real Assets**: All 14,000+ securities from authentic OpenFIGI dataset
- **Industry-Standard Architecture**: Professional asset management data model following best practices
- **Authentic Identifiers**: TICKER + real Bloomberg FIGI identifiers only
- **Performance-Optimized**: SQL-first approach with efficient Snowpark operations  
- **Comprehensive Coverage**: Complete fact/dimension model with proper relationships

**Naming Standards**: See @naming-conventions.mdc for complete naming guidelines.

## Critical Design Principle: OpenFIGI vs Ticker Identifiers

### **ðŸŽ¯ Fundamental Rule: Tickers Are NOT Unique**

**CRITICAL**: Never assume tickers are unique identifiers. The same ticker can represent multiple different assets across markets, exchanges, and instrument types.

#### **âœ… Correct Approach:**
```python
# Load ALL assets that meet quality criteria - do NOT deduplicate by ticker
category_assets = real_assets_df[
    (real_assets_df['ASSET_CATEGORY'] == asset_category) &
    (real_assets_df['PRIMARY_TICKER'].notna()) &
    (real_assets_df['PRIMARY_TICKER'].str.len() <= 15) &
    (real_assets_df['TOP_LEVEL_OPENFIGI_ID'].notna())
]
# No .drop_duplicates(subset=['PRIMARY_TICKER']) - this would remove valid assets!
```

#### **âŒ Wrong Approach:**
```python
# NEVER do this - removes valid assets due to ticker collisions
category_assets = (...).drop_duplicates(subset=['PRIMARY_TICKER'], keep='first')
```

### **Real-World Example: CMC Ticker Collision**
The ticker `CMC` represents multiple companies:
- `BBG001S5PXG8` â†’ Commercial Metals Co (USA) âœ… *Demo scenario company*
- `BBG00BLXJ2S4` â†’ Chaoprayamahanakorn PCL (Thailand)
- `BBG001T6CS16` â†’ Cielo Waste Solutions Corp (Canada)
- `BBG001SSGJ60` â†’ CMC Investment JSC (Vietnam)
- `BBG007MM4S14` â†’ Comeco SA (Poland)

**If you deduplicate by ticker, you lose 4 valid companies and might keep the wrong one!**

### **Design Principles:**
1. **Tickers**: Non-unique, used for display and human reference
2. **OpenFIGI IDs**: Globally unique, used for precise asset identification
3. **Asset Loading**: Keep ALL assets, let OpenFIGI-based selection handle precision
4. **Portfolio Selection**: Use OpenFIGI IDs for exact company matching
5. **Demo Scenarios**: Always specify exact companies via OpenFIGI IDs

### **Implementation Pattern:**
```python
# 1. Load all assets (no ticker deduplication)
all_assets = load_all_real_assets_meeting_criteria()

# 2. Use OpenFIGI IDs for precise selection
demo_companies = {
    'AAPL': 'BBG001S5N8V8',  # Apple Inc. (not Apple Inc-CDR)
    'CMC': 'BBG001S5PXG8',   # Commercial Metals Co (not Thai company)
    'RBBN': 'BBG00HW4CSH5'   # Ribbon Communications Inc.
}

# 3. Portfolio logic uses OpenFIGI for exact matching
WHEN s.FIGI = 'BBG001S5PXG8' THEN 1  -- Exact company, no ambiguity
```

## Enhanced Data Model Architecture

### 1.1 Core Dimension Tables (Industry Standard)
```sql
-- Master security dimension with immutable SecurityID and direct identifiers
DIM_SECURITY (
    SecurityID BIGINT IDENTITY(1,1) PRIMARY KEY,  -- Immutable surrogate key
    IssuerID BIGINT NOT NULL,                     -- FK to DIM_ISSUER
    Ticker VARCHAR(50) NOT NULL,                  -- Real ticker from OpenFIGI dataset
    FIGI VARCHAR(50),                             -- Real FIGI from OpenFIGI (only authentic external ID)
    Description VARCHAR(255),
    AssetClass VARCHAR(50),                       -- 'Equity', 'Corporate Bond', 'ETF'
    SecurityType VARCHAR(100),
    CountryOfRisk CHAR(2),
    IssueDate DATE,
    MaturityDate DATE,                            -- For bonds
    CouponRate DECIMAL(18, 8),                    -- For bonds
    RecordStartDate TIMESTAMP_NTZ,
    RecordEndDate TIMESTAMP_NTZ,
    IsActive BOOLEAN
)

-- Issuer dimension with corporate hierarchies
DIM_ISSUER (
    IssuerID BIGINT IDENTITY(1,1) PRIMARY KEY,
    UltimateParentIssuerID BIGINT,                -- Self-referencing for hierarchy
    LegalName VARCHAR(255) NOT NULL,
    LEI VARCHAR(20),                              -- Legal Entity Identifier
    CountryOfIncorporation CHAR(2),
    GICS_Sector VARCHAR(100)
)

-- Portfolio and benchmark dimensions
DIM_PORTFOLIO (
    PortfolioID BIGINT IDENTITY(1,1) PRIMARY KEY,
    PortfolioCode VARCHAR(100) UNIQUE NOT NULL,
    PortfolioName VARCHAR(255),
    Strategy VARCHAR(100),
    BaseCurrency CHAR(3),
    InceptionDate DATE
)

DIM_BENCHMARK (
    BenchmarkID BIGINT IDENTITY(1,1) PRIMARY KEY,
    BenchmarkName VARCHAR(255) UNIQUE NOT NULL,
    Provider VARCHAR(100)
)
```

### 1.2 Core Fact Tables (Transaction-Based Model)
```sql
-- Canonical transaction log (source of truth)
FACT_TRANSACTION (
    TransactionID BIGINT IDENTITY(1,1) PRIMARY KEY,
    TransactionDate DATE NOT NULL,                -- Direct date column
    PortfolioID BIGINT NOT NULL,
    SecurityID BIGINT NOT NULL,
    TransactionType VARCHAR(50) NOT NULL,         -- 'BUY', 'SELL', 'DIVIDEND', 'INTEREST'
    TradeDate DATE NOT NULL,
    SettleDate DATE,
    Quantity DECIMAL(38, 10),
    Price DECIMAL(38, 10),
    GrossAmount_Local DECIMAL(38, 10),
    Commission_Local DECIMAL(38, 10),
    Currency CHAR(3),
    SourceSystem VARCHAR(50)                      -- 'ABOR' or 'IBOR'
)

-- ABOR positions (built from transactions)
FACT_POSITION_DAILY_ABOR (
    HoldingDate DATE NOT NULL,                    -- Direct date column
    PortfolioID BIGINT NOT NULL,
    SecurityID BIGINT NOT NULL,
    Quantity DECIMAL(38, 10),
    MarketValue_Local DECIMAL(38, 10),
    MarketValue_Base DECIMAL(38, 10),
    CostBasis_Local DECIMAL(38, 10),
    CostBasis_Base DECIMAL(38, 10),
    AccruedInterest_Local DECIMAL(38, 10),
    PortfolioWeight DECIMAL(18, 12)
)

-- Enhanced market data
FACT_MARKETDATA_TIMESERIES (
    PriceDate DATE NOT NULL,                      -- Direct date column
    SecurityID BIGINT NOT NULL,
    Price_Close DECIMAL(38, 10),
    Price_Open DECIMAL(38, 10),
    Price_High DECIMAL(38, 10),
    Price_Low DECIMAL(38, 10),
    Volume BIGINT,
    TotalReturnFactor_Daily DECIMAL(38, 15)
)
```

## Step 2: Data Generation Patterns

### 2.1 Foundation Table Creation Order
```python
def build_foundation_tables(session: Session, test_mode: bool = False):
    """Build all foundation tables in dependency order."""
    
    # Step 1: Build issuer dimension first (no dependencies)
    build_dim_issuer(session, test_mode)
    
    # Step 2: Build security dimension with direct identifiers (depends on issuers)
    build_dim_security(session, test_mode)
    
    # Step 3: Build portfolio and benchmark dimensions
    build_dim_portfolio(session)
    build_dim_benchmark(session)
    
    # Step 4: Build transaction log (depends on portfolios and securities)
    build_fact_transaction(session, test_mode)
    
    # Step 5: Build ABOR positions from transactions
    build_fact_position_daily_abor(session)
    
    # Step 6: Build market data with real data integration
    build_fact_marketdata_timeseries(session, test_mode)
    
    # Step 7: Build remaining fact tables
    build_fundamentals_and_estimates(session)
    build_esg_scores(session)
    build_factor_exposures(session)
    build_benchmark_holdings(session)
```

### 2.2 Real Asset Data Integration (Enhanced)

**âš ï¸ CRITICAL**: Follow the OpenFIGI vs Ticker principle above - never deduplicate by ticker when loading real assets!

```python
# Enhanced real asset integration with issuer mapping
def build_dim_issuer_from_real_data(session: Session):
    """Build issuer dimension from real asset data."""
    
    issuer_data = []
    for ticker, company_data in config.REAL_ASSET_ISSUER_MAPPING.items():
        issuer_data.append({
            'LegalName': company_data['legal_name'],
            'CountryOfIncorporation': company_data['country'],
            'GICS_Sector': company_data['sector'],
            'LEI': f"LEI{hash(company_data['legal_name']) % 1000000:06d}"
        })
    
    issuers_df = session.create_dataframe(issuer_data)
    issuers_df.write.mode("overwrite").save_as_table(f"{config.DATABASE_NAME}.CURATED.DIM_ISSUER")

def build_security_from_real_data(session: Session, real_assets_df: pd.DataFrame):
    """Build securities with direct TICKER and FIGI columns from real asset data."""
    
    security_data = []
    for security_id, asset in enumerate(real_assets, 1):
        # Create security record with direct identifier columns
        security_data.append({
            'SecurityID': security_id,
            'IssuerID': asset['issuer_id'],
            'Ticker': asset['PRIMARY_TICKER'],
            'FIGI': asset.get('TOP_LEVEL_OPENFIGI_ID', f"BBG{abs(hash(asset['PRIMARY_TICKER'])) % 1000000:06d}"),
            'Description': asset['SECURITY_NAME'],
            'AssetClass': asset['ASSET_CATEGORY'],
            'IsActive': True
        })
```

### 2.3 Transaction-Based Holdings Generation
```python
def build_fact_transaction(session: Session, test_mode: bool = False):
    """Generate synthetic transaction history that builds to current positions."""
    
    # Generate realistic transaction patterns over 12 months
    session.sql(f"""
        CREATE OR REPLACE TABLE {config.DATABASE_NAME}.CURATED.FACT_TRANSACTION AS
        WITH portfolio_securities AS (
            -- Select securities for each portfolio based on strategy
            SELECT p.PortfolioID, s.SecurityID
            FROM {config.DATABASE_NAME}.CURATED.DIM_PORTFOLIO p
            CROSS JOIN {config.DATABASE_NAME}.CURATED.DIM_SECURITY s
            WHERE CASE 
                WHEN p.PortfolioName LIKE '%Tech%' THEN i.GICS_Sector = 'Information Technology'
                WHEN p.PortfolioName LIKE '%Multi-Asset%' THEN TRUE
                ELSE s.AssetClass = 'Equity'
            END
        ),
        transaction_dates AS (
            -- Generate weekly transaction dates over 12 months
            SELECT DATEADD(day, seq4() * 7, DATEADD(month, -{config.SYNTHETIC_TRANSACTION_MONTHS}, CURRENT_DATE())) as trade_date
            FROM TABLE(GENERATOR(rowcount => {config.SYNTHETIC_TRANSACTION_MONTHS * 4}))
            WHERE DAYOFWEEK(trade_date) BETWEEN 2 AND 6
        )
        SELECT 
            ROW_NUMBER() OVER (ORDER BY ps.PortfolioID, ps.SecurityID, td.trade_date) as TransactionID,
            td.trade_date as TransactionDate,
            ps.PortfolioID,
            ps.SecurityID,
            'BUY' as TransactionType,
            td.trade_date as TradeDate,
            DATEADD(day, 2, td.trade_date) as SettleDate,
            UNIFORM(100, 10000, RANDOM()) as Quantity,
            UNIFORM(50, 500, RANDOM()) as Price,
            NULL as GrossAmount_Local,
            UNIFORM(5, 50, RANDOM()) as Commission_Local,
            'USD' as Currency,
            'ABOR' as SourceSystem
        FROM portfolio_securities ps
        CROSS JOIN transaction_dates td
        WHERE UNIFORM(0, 1, RANDOM()) < 0.1  -- Sparse transactions
    """).collect()

def build_fact_position_daily_abor(session: Session):
    """Build ABOR positions from transaction log."""
    
    session.sql(f"""
        CREATE OR REPLACE TABLE {config.DATABASE_NAME}.CURATED.FACT_POSITION_DAILY_ABOR AS
        WITH monthly_dates AS (
            SELECT LAST_DAY(DATEADD(month, seq4(), DATEADD(year, -{config.YEARS_OF_HISTORY}, CURRENT_DATE()))) as position_date
            FROM TABLE(GENERATOR(rowcount => {12 * config.YEARS_OF_HISTORY}))
        ),
        transaction_balances AS (
            -- Calculate cumulative positions from transaction log
            SELECT 
                PortfolioID,
                SecurityID,
                SUM(CASE WHEN TransactionType = 'BUY' THEN Quantity ELSE -Quantity END) as TotalQuantity,
                AVG(Price) as AvgPrice
            FROM {config.DATABASE_NAME}.CURATED.FACT_TRANSACTION
            GROUP BY PortfolioID, SecurityID
            HAVING TotalQuantity > 0
        )
        SELECT 
            md.position_date as HoldingDate,
            tb.PortfolioID,
            tb.SecurityID,
            tb.TotalQuantity as Quantity,
            tb.TotalQuantity * tb.AvgPrice as MarketValue_Base,
            -- Calculate portfolio weights
            (tb.TotalQuantity * tb.AvgPrice) / SUM(tb.TotalQuantity * tb.AvgPrice) OVER (PARTITION BY md.position_date, tb.PortfolioID) as PortfolioWeight
        FROM monthly_dates md
        CROSS JOIN transaction_balances tb
    """).collect()
```

## Step 3: Market Data Generation (Synthetic Only)

### 3.1 Synthetic Market Data Generation
```python
def build_fact_marketdata_timeseries(session: Session):
    """Build synthetic market data for all securities with realistic patterns."""
    
    # Generate synthetic OHLCV data for all securities
    session.sql(f"""
        CREATE OR REPLACE TABLE {config.DATABASE_NAME}.CURATED.FACT_MARKETDATA_TIMESERIES AS
        WITH business_dates AS (
            SELECT DATEADD(day, seq4(), DATEADD(year, -{config.YEARS_OF_HISTORY}, CURRENT_DATE())) as price_date
            FROM TABLE(GENERATOR(rowcount => {365 * config.YEARS_OF_HISTORY}))
            WHERE DAYOFWEEK(price_date) BETWEEN 2 AND 6
        ),
        securities_dates AS (
            SELECT 
                s.SecurityID,
                bd.price_date as PriceDate
            FROM {config.DATABASE_NAME}.CURATED.DIM_SECURITY s
            CROSS JOIN business_dates bd
        )
        SELECT 
            sd.PriceDate,
            sd.SecurityID,
            -- Generate realistic synthetic prices with volatility
            UNIFORM(50, 500, RANDOM()) as Price_Close,
            UNIFORM(50, 500, RANDOM()) as Price_Open,
            UNIFORM(50, 500, RANDOM()) as Price_High,
            UNIFORM(50, 500, RANDOM()) as Price_Low,
            UNIFORM(1000, 1000000, RANDOM()) as Volume,
            1.0 as TotalReturnFactor_Daily
        FROM securities_dates sd
    """).collect()
        
        # Build market data with SecurityID mapping
        session.sql(f"""
            CREATE OR REPLACE TABLE {config.DATABASE_NAME}.CURATED.FACT_MARKETDATA_TIMESERIES AS
            WITH business_dates AS (
                SELECT DATEADD(day, seq4(), DATEADD(year, -{config.YEARS_OF_HISTORY}, CURRENT_DATE())) as price_date
                FROM TABLE(GENERATOR(rowcount => {365 * config.YEARS_OF_HISTORY}))
                WHERE DAYOFWEEK(price_date) BETWEEN 2 AND 6
            ),
            securities_dates AS (
                SELECT 
                    s.SecurityID,
                    s.Ticker,
                    s.AssetClass,
                    bd.price_date as PriceDate
                FROM {config.DATABASE_NAME}.CURATED.DIM_SECURITY s
                CROSS JOIN business_dates bd
            ),
            real_data AS (
                SELECT 
                    TICKER,
                    DATE::date as PRICE_DATE,
                    CLOSE_PRICE,
                    OPEN_PRICE,
                    HIGH_PRICE,
                    LOW_PRICE,
                    VOLUME
                FROM {config.DATABASE_NAME}.RAW.TEMP_REAL_MARKET_DATA
                WHERE CLOSE_PRICE IS NOT NULL
            )
            SELECT 
                sd.PriceDate,
                sd.SecurityID,
                -- Use real data when available, synthetic otherwise
                COALESCE(rd.CLOSE_PRICE, synthetic_price_generation) as Price_Close,
                COALESCE(rd.OPEN_PRICE, synthetic_price_generation) as Price_Open,
                COALESCE(rd.HIGH_PRICE, synthetic_price_generation) as Price_High,
                COALESCE(rd.LOW_PRICE, synthetic_price_generation) as Price_Low,
                COALESCE(rd.VOLUME, synthetic_volume_generation) as Volume,
                1.0 as TotalReturnFactor_Daily
            FROM securities_dates sd
            LEFT JOIN real_data rd ON sd.TICKER = rd.TICKER AND sd.PriceDate = rd.PRICE_DATE
        """).collect()
```

### 3.2 Configuration Control (Enhanced)
```python
# Enhanced configuration in config.py
USE_TRANSACTION_BASED_MODEL = True
GENERATE_CORPORATE_HIERARCHIES = True
ISSUER_HIERARCHY_DEPTH = 2

# Transaction generation settings
SYNTHETIC_TRANSACTION_MONTHS = 12
TRANSACTION_TYPES = ['BUY', 'SELL', 'DIVIDEND', 'CORPORATE_ACTION']
AVERAGE_MONTHLY_TRANSACTIONS_PER_SECURITY = 2.5

# Real data integration (preserved)
USE_REAL_ASSETS_CSV = True
REAL_ASSETS_CSV_PATH = '../data/real_assets.csv'
# Market data: Synthetic generation for all securities

# Enhanced real asset to issuer mapping
REAL_ASSET_ISSUER_MAPPING = {
    'AAPL': {'legal_name': 'Apple Inc.', 'country': 'US', 'sector': 'Information Technology'},
    'MSFT': {'legal_name': 'Microsoft Corporation', 'country': 'US', 'sector': 'Information Technology'},
    # ... additional mappings
}
```

## Step 4: Enhanced Document Integration

### 4.1 SecurityID-Based Document Linkage
```sql
-- Enhanced document corpus schema
{DOCUMENT_TYPE}_CORPUS (
    DOCUMENT_ID VARCHAR PRIMARY KEY,
    DOCUMENT_TITLE VARCHAR(500),
    DOCUMENT_TYPE VARCHAR(100),
    SecurityID BIGINT,                            -- FK to DIM_SECURITY
    IssuerID BIGINT,                              -- FK to DIM_ISSUER  
    PUBLISH_DATE DATE,
    LANGUAGE VARCHAR(10) DEFAULT 'en',
    DOCUMENT_TEXT TEXT
)
```

### 4.2 Document Generation with Enhanced Model

**For complete unstructured data generation patterns, see @unstructured-data-generation.mdc**

This section covers document corpus integration with the SecurityID-based model:
- Security-level documents: Linked via SecurityID + IssuerID
- Issuer-level documents: Linked via IssuerID only  
- Global documents: No specific linkage required
- All document types follow standardized patterns defined in the dedicated unstructured data generation rule

## Step 5: Data Quality and Validation

### 5.1 Enhanced Validation Patterns
```python
def validate_enhanced_data_quality(session: Session):
    """Validate data quality of the enhanced model."""
    
    # 1. Validate portfolio weights sum to 100%
    weight_check = session.sql(f"""
        SELECT 
            PortfolioID,
            SUM(PortfolioWeight) as TotalWeight,
            ABS(SUM(PortfolioWeight) - 1.0) as WeightDeviation
        FROM {config.DATABASE_NAME}.CURATED.FACT_POSITION_DAILY_ABOR 
        WHERE HoldingDate = (SELECT MAX(HoldingDate) FROM {config.DATABASE_NAME}.CURATED.FACT_POSITION_DAILY_ABOR)
        GROUP BY PortfolioID
        HAVING ABS(SUM(PortfolioWeight) - 1.0) > 0.001
    """).collect()
    
    # 2. Validate transaction log balances to positions
    transaction_balance_check = session.sql(f"""
        WITH transaction_balances AS (
            SELECT 
                PortfolioID,
                SecurityID,
                SUM(CASE WHEN TransactionType IN ('BUY') THEN Quantity
                         WHEN TransactionType IN ('SELL') THEN -Quantity
                         ELSE 0 END) as TransactionQuantity
            FROM {config.DATABASE_NAME}.CURATED.FACT_TRANSACTION
            WHERE SettleDate <= CURRENT_DATE
            GROUP BY PortfolioID, SecurityID
        )
        SELECT COUNT(*) as mismatches
        FROM {config.DATABASE_NAME}.CURATED.FACT_POSITION_DAILY_ABOR p
        JOIN transaction_balances tb ON p.PortfolioID = tb.PortfolioID AND p.SecurityID = tb.SecurityID
        WHERE p.HoldingDate = (SELECT MAX(HoldingDate) FROM {config.DATABASE_NAME}.CURATED.FACT_POSITION_DAILY_ABOR)
        AND ABS(p.Quantity - tb.TransactionQuantity) > 0.01
    """).collect()
    
    # 3. Validate security identifier integrity (simplified - direct columns)
    security_check = session.sql(f"""
        SELECT 
            COUNT(*) as total_securities,
            COUNT(CASE WHEN Ticker IS NOT NULL AND LENGTH(Ticker) > 0 THEN 1 END) as securities_with_ticker,
            COUNT(CASE WHEN FIGI IS NOT NULL AND LENGTH(FIGI) > 0 THEN 1 END) as securities_with_figi
        FROM {config.DATABASE_NAME}.CURATED.DIM_SECURITY
    """).collect()
```

### 5.2 Required Validations (Enhanced)
- âœ… Portfolio weights sum to 100% (Â±0.1% tolerance)
- âœ… Transaction log balances to position snapshots
- âœ… Security identifier integrity (TICKER and FIGI columns populated)
- âœ… Issuer hierarchy relationships valid
- âœ… No negative prices or market values
- âœ… Date ranges logical and consistent
- âœ… All foreign key relationships valid

## Step 6: Performance and Scalability

### 6.1 Snowpark Optimization Patterns
```python
# Use SQL for complex transformations, Snowpark for data movement
session.sql(complex_transformation_sql).collect()  # Preferred for large datasets

# Use overwrite mode for deterministic builds
df.write.mode("overwrite").save_as_table("SAM_DEMO.CURATED.TABLE_NAME")

# Batch operations for better performance
session.sql("BEGIN").collect()
# ... multiple related operations
session.sql("COMMIT").collect()
```

### 6.2 Memory Management
```python
# For large datasets, use SQL-based generation
def build_large_fact_table(session: Session):
    """Use SQL for large fact table generation"""
    session.sql(f"""
        CREATE OR REPLACE TABLE {table_name} AS
        WITH complex_logic AS (
            -- Complex transformations in SQL
        )
        SELECT * FROM complex_logic
    """).collect()

# Avoid large Pandas DataFrames in memory
# Use session.create_dataframe() for small reference data only
```

## Summary

This guide provides the patterns and procedures for generating industry-standard asset management data with immutable SecurityID, transaction audit trails, issuer hierarchies, and real data integration capabilities.

**Enhanced Model Benefits:**
- Professional asset management data architecture
- Corporate action resilience and temporal integrity  
- Complete transaction audit trail for compliance
- Issuer-level risk analysis and corporate hierarchies
- Real market data integration with synthetic fallback

**Related Rules**: 
- @real-assets.mdc - For real data integration patterns
- @naming-conventions.mdc - For database naming standards
- @development-patterns.mdc - For extending data model